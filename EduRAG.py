import os
import streamlit as st
from langchain.chains import LLMChain
from langchain_groq import ChatGroq
from langchain_mistralai import ChatMistralAI
from langchain_core.messages import SystemMessage
from Project.retriever import format_results, get_common_results
from langchain.memory import ConversationBufferWindowMemory
from Project.vector_pipeline import connect_db
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder, HumanMessagePromptTemplate, PromptTemplate



groq_api_key = "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"

system_prompt = "you are a helpful chatbot for a university student. You are to provide the relevnt {output_data} to help students ."


st.title("ðŸ’¬ EduRAG")
st.caption("ðŸš€ Get ESSEC info without the headache")

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "How can I help you?"}]

if "mistral_chat" not in st.session_state:
    st.session_state.mistral_chat = ChatMistralAI(api_key=api_key, streaming=True, system=system_prompt_template)

if "groq_chat" not in st.session_state:
    st.session_state.groq_chat = ChatGroq(
        groq_api_key=groq_api_key,
        model_name="mixtral-8x7b-32768"
    )

if "memory" not in st.session_state:
    st.session_state.memory = ConversationBufferWindowMemory(k=10, memory_key="chat_history", return_messages=True)

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "human", "content": prompt})
    st.chat_message("user").write(prompt)


    # Create a conversation chain using the LangChain LLM (Language Learning Model)
    connection, cursor = connect_db()
    results = get_common_results(prompt, 'edu', cursor=cursor)
    context = format_results(results)

    custom_answer_prompt_template = """
    <s>[INST] Using the context below:
    context: {context}
    Answer the question: {query}
    Follow the guidelines:
    [/INST]
    """
    answer_prompt = PromptTemplate(template=custom_answer_prompt_template, input_variables=['context', 'query'])
    formatted_prompt = answer_prompt.format(query=prompt, context=context)

    response = st.session_state.mistral_chat.invoke(formatted_prompt)
    st.session_state.messages.append({"role": "AI", "content": response})
    st.chat_message("assistant").write(response)







    # # Construct a chat prompt template using various components
    # prompt_template = ChatPromptTemplate.from_messages(
    #     [
    #         SystemMessage(content=system_prompt),
    #         MessagesPlaceholder(variable_name="chat_history"),
    #         HumanMessagePromptTemplate.from_template("{human_input}"),
    #         # DataOutputPlaceholder(variable_name="output_data"),
    #     ]
    # )

    # # Create a conversation chain using the LangChain LLM (Language Learning Model)
    # conversation = LLMChain(
    #     llm=st.session_state.groq_chat,
    #     prompt=prompt_template,
    #     verbose=True,
    #     memory=st.session_state.memory,
    # )

    # # The chatbot's answer is generated by sending the full prompt to the Groq API.
    # response = conversation.predict(human_input=prompt)

    # st.session_state.messages.append({"role": "AI", "content": response})
    # st.chat_message("assistant").write(response)